<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[test_my_site]]></title>
    <url>%2F2018%2F07%2F17%2Ftest-my-site%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F07%2F17%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[Kuberbetes-- 利用Jenkins在Kubernetes中实践CI/CD]]></title>
    <url>%2F2017%2F12%2F17%2F2017-12-17-achieve-cicd-in-kubernetes-with-jenkins%2F</url>
    <content type="text"><![CDATA[概述本文利用jenkins在k8s中简单实践了一下CI/CD，部分实验内容来自Set Up a CI/CD Pipeline with Kubernetes ，除此外，还试验了一把利用jenkins kubernetes plugin实现动态分配资源构建。 在kubernetes中简单实践jenkins首先简单介绍下jenkins,jenkins是一个java编写的开源的持续集成工具。具体来说，他可以将软件构建，测试，发布等一系列流程自动化，达到一键部署的目的。在进行本实验前，首先要有一个k8s环境，这里不再赘述。 部署jenkins这里存储用的是ceph rbd，所以先创建一个PVC： jenkins-pvc.yaml123456789101112kind: PersistentVolumeClaimapiVersion: v1metadata: name: jenkins-pvc namespace: cicdspec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi storageClassName: ceph-web 部署jenkins: jenkins-deployment.yaml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: jenkins namespace: cicd labels: app: jenkinsspec: strategy: type: Recreate template: metadata: labels: app: jenkins tier: jenkins spec: containers: - image: chadmoon/jenkins-docker-kubectl:latest name: jenkins securityContext: privileged: true ports: - containerPort: 8080 name: jenkins - containerPort: 50000 name: agent protocol: TCP volumeMounts: - name: docker mountPath: /var/run/docker.sock - name: jenkins-persistent-storage mountPath: /root/.jenkins - name: kube-config mountPath: /root/.kube/config - name: image-registry mountPath: /root/.docker volumes: - name: docker hostPath: path: /var/run/docker.sock - name: jenkins-persistent-storage persistentVolumeClaim: claimName: jenkins-pvc - name: kube-config hostPath: path: /root/.kube/config - name: image-registry configMap: name: image-registry-auth 简单解释一下： 该镜像除了安装jenkins，还装了docker cli（与host docker daemon交互），kubectl（与k8s apiserver交互） 容器开了两个端口，一个用于web-ui,一个用于后面实验jenkins kubernetes plugin时与JNLP slave agents 交互。 挂载了四个volume，依次是，一个用于docker cli，一个用于存储jenkins数据，一个用于kubectl与k8s交互验证，最后挂载了一个configmap，与image registry（我们用的harbor）交互验证。 部署jenkins service &amp; ingress: jenkins-service-ingress.yaml1234567891011121314151617181920212223242526272829303132apiVersion: v1kind: Servicemetadata: name: jenkins-web-ui namespace: cicd labels: app: jenkinsspec: ports: - port: 80 targetPort: 8080 name: web-ui - port: 50000 targetPort: 50000 name: agent selector: app: jenkins tier: jenkins---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: jenkins-web-ui namespace: cicdspec: rules: - host: jenkins.com http: paths: - backend: serviceName: jenkins-web-ui servicePort: 80 完成上述所有操作后，查看一下对应的pod 是否running。 配置pipeline按照ingress的配置，修改本地host，然后浏览器中输入http://jenkins.com ，就进入到jenkins 的web-ui了。 按照提示创建完用户后，开始进入CICD 的实验环节： 新建一个item，命名并选中pipeline: pipeline 配置如下： 在Git Repository URL部分添加github url,这里用的是我的github-test-kubernetes-ci-cd，是直接fork自kubernetes-ci-cd，并做了一些更改,之后保存就可以了。进入刚创建的item，点击立即构建： 之后就可以看到构建信息了，如果出错也可以查看对应步骤的log。同时，我们的应用也已经部署到k8s中了。 步骤详解接下来看一下点击“立即构建”后发生了什么，点击后，jenkins首先是从github检出项目代码，然后根据检出的项目中根目录下的Jenkinsfile进行项目构建，看下该项目的Jenkinsfile。123456789101112131415161718192021222324252627node &#123; checkout scm env.DOCKER_API_VERSION="1.23" sh "git rev-parse --short HEAD &gt; commit-id" tag = readFile('commit-id').replace("\n", "").replace("\r", "") appName = "hello-kenzan" registryHost = "172.16.21.253:10080/library/" imageName = "$&#123;registryHost&#125;$&#123;appName&#125;:$&#123;tag&#125;" env.BUILDIMG=imageName stage "Build" sh "docker build -t $&#123;imageName&#125; -f applications/hello-kenzan/Dockerfile applications/hello-kenzan" stage "Push" sh "docker push $&#123;imageName&#125;" stage "Deploy" sh "sed 's#127.0.0.1:30400/hello-kenzan:latest#'$BUILDIMG'#' applications/hello-kenzan/k8s/deployment.yaml | kubectl apply -f -" sh "kubectl rollout status deployment/hello-kenzan"&#125; 可以看到，Jenkinsfile定义了三个阶段，第一个阶段是“Build”,这个阶段是根据给定的Dockerfile创建一个镜像，第二个阶段“Push”,把生成的镜像push到我们的镜像仓库中，最后一个阶段是”Deploy”，编辑了一下deployment.yaml模板，然后调用kubectl命令进行部署。看一下“Build”阶段的dockerfile:123456FROM nginx:latestCOPY index.html /usr/share/nginx/html/index.htmlCOPY DockerFileEx.jpg /usr/share/nginx/html/DockerFileEx.jpgEXPOSE 80 就是一个很简单的nginx应用。再看下deployment.yaml：12345678910111213141516171819202122232425262728293031323334353637apiVersion: v1kind: Servicemetadata: name: hello-kenzan labels: app: hello-kenzanspec: ports: - port: 80 targetPort: 80 selector: app: hello-kenzan tier: hello-kenzan type: NodePort---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: hello-kenzan labels: app: hello-kenzanspec: strategy: type: Recreate template: metadata: labels: app: hello-kenzan tier: hello-kenzan spec: containers: - image: 127.0.0.1:30400/hello-kenzan:latest name: hello-kenzan ports: - containerPort: 80 name: hello-kenzan 这里服务发现使用的nodeport，可以改为其他方式（比如ingress）。 接下来，可以试着修改一下index.html，然后push到github中，再构建一下，看看内容是否改变。 利用jenkins kubernetes plugin实现动态分配资源构建在上述实例中，我们利用jenkins实现了一个小应用的CI/CD，这个小应用非常简单，“build”阶段就是直接在本地调用host docker构建的镜像，设想一下，如果这个应用需要编译，需要测试，那么这个时间就长了，而且如果都在本地构建的话，一个人使用还好，如果多个人一起构建，就会造成拥塞。为了解决上述问题，我们可以充分利用k8s的容器编排功能，jenkins接收到任务后，调用k8s api，创造新的 agent pod，将任务分发给这些agent pod，agent pod执行任务，任务完成后将结果汇总给jenkins pod，同时删除完成任务的agent pod。为了实现上述功能，我们需要给jenkins安装一个插件，叫做jenkins kubernetes plugin。 插件安装与配置安装比较简单，直接到jenkins 界面的系统管理，插件管理界面进行安装就可以了。安装好之后，进入系统管理—–&gt;系统设置，最下面有一个“云”，选择“新增一个云”—-&gt;kubernetes。 这里没有配置k8s，因为如果不配置api-server的话，jenkins会默认使用~/.kube/config下的配置，而我们已经在~/.kube/config做过配置了，所以这里就不做了。Jenkins URL我们使用的是集群内的服务地址。再往下看 kubernetes pod template配置： 这个pod tempalte就是之后我们创建 agent使用的模板，镜像使用“jenkins/jnlp-slave:alpine”，配置完成后，点击保存。然后还要配置一下agent与jenkins通信的端口，在系统管理—-&gt;Configure Global Security，指定端口为我们之前设定的5000端口： 简单测试配置完成后做一个简单的测试。 新建一个item，这里选择“构建一个自由风格的软件项目”： 配置时注意在General部分有一个restrict： Label Expression就写之前我们k8s podtemplate 的label。 在构建部分我们写一个简单的测试命令：echo TRUE 点击立即构建，如果成功的话，我们在“管理主机”模块会看到新增了一个主机： 同时，也会在k8s中发现新创建了一个名为jnlp-slave-8bq5m的pod。任务结束后，pod删除，主机消失，在console output 会看到执行结果： 出现问题总结jnlp-slave pod创建失败查看pod日志，发现是连接不上jenkins ，通过修改Configure Global Security的启用安全，TCP port for JNLP agents指定端口解决。 jnlp-slave pod 无法删除因为我们执行构建后，如果 jnlp-slave pod创建失败，它会不断的尝试创建新的pod，并试图连接jenkins，一段时间后，就会创造很多失败的jnlp-slave pod。如果遇到这种情况，需要尽早中断任务并删除失败的pod。在删除某个pod时 ，该pod一直处于termating阶段，kubectl delete无法删除。后来参考Pods stuck at terminated status，使用如下命令解决：1kubectl delete pod NAME --grace-period=0 --force 参考文章Set Up a CI/CD Pipeline with a Jenkins Pod in Kubernetes Achieving CI/CD with Kubernetes 容器时代CI/CD平台中的Kubernetes调度器定制方法 Jenkinsfile使用 安装和设置 kubectl jenkins-kubernetes-plugin 基于Kubernetes 部署 jenkins 并动态分配资源 使用Kubernetes-Jenkins实现CI/CD 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph-- ceph rbd快照的实现原理]]></title>
    <url>%2F2017%2F06%2F05%2Fceph-rbd-snapshot%2F</url>
    <content type="text"><![CDATA[引出关于openstack 中的快照，备份部分之前做过一次总结，openstack 中的快照与备份 ,因为目前基本使用ceph rbd作为共享存储后端，所以本篇文章想讨论以rbd作为存储后端时的备份与快照情况： nova image-create：具体实现参考openstack 中的快照与备份 nova backup: 同上。 cinder snapshot：直接利用 rbd snapshot 实现 cinder backup：支持增量备份，也用到了rbd snapshot，参考 cinder-backup 利用ceph实现增量备份 由以上情况可知，rbd snapshot 几乎都要用到，所以接下来探讨下rbd snapshot 的实现原理。 rbd snapshot vs rbd clone首先知道这两者的区别： snapshot：是某一镜像(image) 在特定时间点的一个只读副本,注意，是只读副本，所以该snapshot是无法作为一个image去进行写操作的（比如起一个虚拟机）。 clone: 是针对snapshot的一种操作，可以理解为对snapshot的再度拷贝，从而实现可写操作，简单讲就是将image的某一个snapshot 的状态复制变成另一个image，该image状态与snapshot完全一致，但是可读可写（可以从此image起一个虚拟机）。 弄懂了两者的区别，接下来需要认真探讨下两者实现的原理： rbd snapshot 实现原理注意：snapshot时，一定要对原rbd image 停止IO操作，否则会引起数据不一致。 跟git 实现版本控制类似，使用 COW （copy on write）方式实现。这里叙述下对某一pool中的某个image 进行snapshot 的过程： 直接引用自解析Ceph: Snapshot： 每个Pool都有一个snap_seq字段，该字段可以认为是整个Pool的Global Version。所有存储在Ceph的Object也都带有snap_seq，而每个Object会有一个Head版本的，也可能会存在一组Snapshot objects，不管是Head版本还是snapshot object都会带有snap_seq，那么接下来我们看librbd是如何利用该字段创建Snapshot的。 用户申请为”pool”中的”image-name”创建一个名为”snap-name”的Snapshot librbd向Ceph Monitor申请得到一个”pool”的snap sequence，Ceph Monitor会递增该Pool的snap_seq，然后返回该值给librbd。 librbd将新的snap_seq替换原来image的snap_seq中，并且将原来的snap_seq设置为用户创建的名为”snap-name”的Snapshot的snap_seq。 每个Snapshot都掌握者一个snap_seq，Image可以看成一个Head Version的Snapshot，每次IO操作对会带上snap_seq发送给Ceph OSD，Ceph OSD会查询该IO操作涉及的object的snap_seq情况。如”object-1″是”image-name”中的一个数据对象，那么初始的snap_seq就”image-name”的snap_seq，当创建一个Snapshot以后，再次对”object-1″进行写操作时会带上新的snap_seq，Ceph接到请求后会先检查”object-1″的Head Version，会发现该写操作所带有的snap_seq大于”object-1″的snap_seq，那么就会对原来的”object-1″克隆一个新的Object Head Version，原来的”object-1″会作为Snapshot，新的Object Head会带上新的snap_seq，也就是librbd之前申请到的。 实验过程可以参考理解 OpenStack + Ceph （4）：Ceph 的基础数据结构 [Pool, Image, Snapshot, Clone],实验结论摘抄如下： rbd clone 实现原理从用户角度来说，clone操作实现了对某个只读snapshot 的image化，即clone之后，可以对这个clone进行读写，snapshot等等，跟一个rbd image一样。从系统实现来说，也是利用 COW方式实现，clone会将clone与snapshot 的父子关系保存起来,以备IO操作时查找。 实验验证部分参考理解 OpenStack + Ceph （4）：Ceph 的基础数据结构 [Pool, Image, Snapshot, Clone]。 结论摘抄如下： COW VS ROW要想了解COW 与ROW，首先需要知道快照的两种类型，一种是全量快照，一种是增量快照。 全量快照，顾名思义，就是为源数据卷创建并维护一个完整的镜像卷。 增量快照，利用COW 或ROW的方式实现的差量快照。 COW(COPY-ON-WRITE) 写时复制在创建快照的时候，仅仅复制一份数据指针表，当读取快照数据时，快照本身没有的话，根据数据指针表，直接读取源数据卷的对应数据块，在更新或写入源数据卷中的数据块时，先将原始数据copy到快照卷中（预留空间），更新快照卷的数据指针表，再将更新数据写入源数据卷。 写时复制的优势在于创建快照速度快（仅复制数据指针表），且占用存储空间少，但因为创建快照后每次写入操作都要进行一次复制才开始写入源数据，所以降低了源数据卷的写性能。所以COW 适合读多写少的场景，除此之外，如果一个应用容易出现对存储设备的写入热点(只针对某个有限范围内的数据进行写操作),也是比较理想的选择.因为其数据更改都局限在一个范围内（局部性原理）, 对同一份数据的多次写操作只会出现一次写时复制操作。 ROW(REDIRECT-ON-WRITE)写时重定向在创建快照的时候，仅仅复制一份数据指针表，当读取快照数据时，快照本身没有的话，根据数据指针表，直接读取源数据卷的对应数据块，在更新或写入源数据卷中的数据块时，将源数据卷数据指针表中的被更新原始数据指针重定向到新的存储空间，所以由此至终, 快照卷的数据指针表和其对应的数据是没有被改变过的。恢复快照的时候,只需要按照快照卷数据指针表来进行寻址就可以完成恢复了. 除了COW的优势外，ROW 因为更新源数据卷只需要一次写操作, 解决了 COW写两次的性能问题. 所以 ROW 最明显的优势就是不会降低源数据卷的写性能。但ROW 的快照卷数据指针表保存的是源数据卷的原始副本,而源数据卷数据指针表保存的则是更新后的副本,导致在删除快照卷之前需要将快照卷数据指针表指向的数据同步至源数据卷中. 而且当创建了多个快照后, 会产生一个快照链,使原始数据的访问、快照卷和源数据卷数据的追踪以及快照的删除将变得异常复杂且消耗时间。除此之外,因为源数据卷数据指针指向的数据会很快的被重定向分散, 所以 ROW 另一个主要缺点就是降低了读性能(局部空间原理)。在传统存储设备上,ROW快照在多次读写后,源数据卷的数据被分散,对于连续读写的性能不如COW. 所以 ROW 比较适合Write-Intensive(写密集) 类型的存储系统. 但是, 在分布式存储设备上, ROW 的连续读写的性能会比 COW 更加好. 一般而言,读写性能的瓶颈都在磁盘上.而分布式存储的特性是数据越是分散到不同的存储设备中, 系统性能越高。所以ROW的源数据卷重定向分散性反而带来了好处。 因此, ROW 逐渐成为了业界分布式存储的主流。 参考文章SNAPSHOTS 解析Ceph: Snapshot 关于Ceph的snapshot和clone Ceph Snapshots: Diving into Deep Waters 理解 OpenStack + Ceph （4）：Ceph 的基础数据结构 [Pool, Image, Snapshot, Clone] ROW/COW 快照技术原理解析 本篇文章由pekingzcc采用知识共享署名-非商业性使用 4.0 国际许可协议进行许可,转载请注明。 END]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>snapshot</tag>
      </tags>
  </entry>
</search>
